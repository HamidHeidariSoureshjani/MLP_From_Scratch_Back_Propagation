{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "g-MBZRFoiXkt"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample = pd.read_csv('/content/drive/MyDrive/iris.data.csv')\n",
        "sample = sample.replace('Iris-setosa', 0)\n",
        "sample = sample.replace('Iris-versicolor', 1)\n",
        "sample = sample.replace('Iris-virginica', 2)\n",
        "sample = sample.to_numpy()\n",
        "X = sample[:, :4]\n",
        "Y = sample[:, 4]"
      ],
      "metadata": {
        "id": "wAN5PmvHQj14"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HT-zOGxFibqw"
      },
      "outputs": [],
      "source": [
        "class MLP(object):\n",
        "\n",
        "  # Define Layers\n",
        "  def __init__(self , num_inputs=1 , num_hidden=[3, 5] , num_outputs=1):\n",
        "    self.num_inputs = num_inputs\n",
        "    self.num_hidden = num_hidden\n",
        "    self.num_outputs = num_outputs\n",
        "\n",
        "    layers = [num_inputs] + num_hidden + [num_outputs]\n",
        "\n",
        "    # Define Weights\n",
        "    weights = []\n",
        "    for i in range (len(layers) - 1):\n",
        "      w = np.random.rand(layers[i], layers[i+1])\n",
        "      weights.append(w)\n",
        "    self.weights = weights\n",
        "\n",
        "    # Define Inputs\n",
        "    activations = []\n",
        "    for i in range(len(layers)):\n",
        "      a = np.zeros(layers[i])\n",
        "      activations.append(a)\n",
        "    self.activations = activations\n",
        "\n",
        "    # Define Derivatives\n",
        "    derivatives = []\n",
        "    for i in range(len(layers) - 1):\n",
        "      d = np.zeros([layers[i] , layers[i+1]])\n",
        "      derivatives.append(d)\n",
        "\n",
        "  # Sigmoid Avtivation Function\n",
        "  def _sigmoid(self , x):\n",
        "    y = 1 / (1 + np.exp(-x))\n",
        "    return y\n",
        "    self.derivatives = derivatives\n",
        "\n",
        "  # Derivative Sigmoid Avtivation Function\n",
        "  def _sigmoid_derivatives(self , x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "  # Forward Propagation\n",
        "  def forward_propagation (self , inputs):\n",
        "    activations = inputs\n",
        "    self.activations[0] = inputs\n",
        "\n",
        "    for i , w in enumerate(self.weights):\n",
        "      net_inputs = np.dot(activations, w)\n",
        "      activations = self._sigmoid(net_inputs)\n",
        "      self.activations[i+1] = activations\n",
        "    return activations\n",
        "\n",
        "  # Back Propagation\n",
        "  def back_propagation(self , error, verbose=False):\n",
        "\n",
        "    for i in reversed( range (len(self.derivatives)) ):\n",
        "      activations = self.activations[i+1]\n",
        "      delta = error * self._sigmoid_derivatives(activations)\n",
        "      delta_reshaped = delta.reshape(delta.shape[0] , -1).T\n",
        "      current_activations = self.activations[i]\n",
        "      current_activations_reshaped = current_activations.reshape(current_activations.shape[0] , -1).T\n",
        "      self.derivatives[i] = np.dot(current_activations_reshaped , delta_reshaped)\n",
        "      error = np.dot(delta, self.weights[i].T)\n",
        "\n",
        "    return error\n",
        "\n",
        "  # Gradient Descent\n",
        "  def gradient_descent(self, learning_rate):\n",
        "\n",
        "    for i in range (len(self.weights)):\n",
        "      weights = self.weights[i]\n",
        "      derivatives = self.derivatives[i]\n",
        "      weights  = weights + derivatives * learning_rate\n",
        "\n",
        "\n",
        "  # Train Model\n",
        "  def train(self, inputs, targets, epochs, learning_rate):\n",
        "\n",
        "    for i in range(epochs):\n",
        "      sum_error = 0\n",
        "      for i in range(X.shape[0]):\n",
        "        sample = X[i]\n",
        "        label = Y[i]\n",
        "      # for input, target in (inputs, targets):\n",
        "\n",
        "        # forward propagation\n",
        "        output = self.forward_propagation(sample)\n",
        "\n",
        "        # calculate the error\n",
        "        error = label - output\n",
        "\n",
        "        # back propagation\n",
        "        self.back_propagation(error)\n",
        "\n",
        "        # apply gradeint descent\n",
        "        self.gradient_descent(learning_rate)\n",
        "\n",
        "        sum_error = sum_error + self.mse(label , output)\n",
        "\n",
        "      # report error\n",
        "      print(\"Error: {} at epoch {}\".format(sum_error/len(inputs) , i))\n",
        "\n",
        "  # Loss Function\n",
        "  def _mse(self, target , output):\n",
        "    return np.average((target - output)**2)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "  # create an mlp\n",
        "  mlp = MLP(num_inputs=2 , num_hidden=[5,2] , num_outputs=1)\n",
        "\n",
        "  # train mlp\n",
        "  mlp.train(X , Y, 50 , 0.1)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}